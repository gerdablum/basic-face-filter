{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real time face filter \n",
    "\n",
    "This project realizes a very basic face filter, similar to the snapchat/instagram filters out there (but much more basic). Per default, your face is replaced by hide-the-pain-harold's face. Custom images can be added of course. See the stunning result above:\n",
    "\n",
    "<img src=\"example.jpg\" width=\"200\"/>\n",
    "\n",
    "The underlying techniques of this mini-project are face-detection library from [MediaPipe](https://github.com/google/mediapipe), Gooogle's live data and streaming media ML project.\n",
    "With openCV and some basic numpy operations, we perform the replacement of the face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting faces\n",
    "\n",
    "For the face recognition, we don't have to do much, as we use the face regognition model of mediapipe.\n",
    "\n",
    "The ``detection`` method uses the model from google's face detection and returns a result.\n",
    "We need to convert the image to RGB first (openCV loads the images as BGR for some reason)\n",
    "To save some memory, we set writeable to false, while processing the image.\n",
    "\n",
    "The result usually contains detections for each face and we can extract keypoints for eyes, nose and mouth as well as acces the raw data (we will need that later for the bounding box).\n",
    "\n",
    "The `draw_face_detection` method is used only for debug reasons. It visualises the bounding box of a face and the \"keypoints\", such as eyes and nose. It's really interesting to see what the library detects, but for the end result we don't need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_face_detection(image, results, mp_drawing):\n",
    "    if results.detections is None:\n",
    "        return\n",
    "    for detection in results.detections:\n",
    "        mp_drawing.draw_detection(image, detection)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the filter\n",
    "\n",
    "In this part, we actually have to do a little work. In `add_face_overlays` we loop over detections. The result object contains a detection for each detected face. If there is no face detection in the image we do an early return. Then we call the actual face overlay method with one face image per detection.\n",
    "\n",
    "The `add_face_overlay_method` performs the following steps:\n",
    "\n",
    "1. create an alphatransparent overlay image with the same size as the original camera frame\n",
    "2. extract the upper left corner point of the face recognition bounding box as well as the width\n",
    "3. scale face image to the right width of the detected face\n",
    "4. place face image at the right position of overlay image\n",
    "5. combine overlay image and original camera frame to one image and return it\n",
    "\n",
    "**1. create an alphatransparent overlay** \\\n",
    "We create an empty image (all zeros) which has 4 channels, the 4th is the transparency information. The image is fully transparent. We will place our face image inside this empty overlay. We could save some memory and computation time, if we add the face image directly, but I found it much easier to combine the face with the background image, if they both have the same size\n",
    "\n",
    "**2. Extract face detection points** \\\n",
    "We have to access the raw data of the detection object, beause we want the upper left corner of the face bounding box to anchor our face image. The raw data coordinates are relative numbers between 0 and 1 so we scale them up to the image width and height. Last we test out and add some values on top to fit the face overlay to the actual face.\n",
    "\n",
    "**3. Scale face image** \\\n",
    "We want that our face image has always the same size as the original face. Therefore we take the widh of the face detection bounding box (plus some threshold) and scale the face image accordingly. We must maintain the width and height properties. The scaling is done in `scale_image`. The width of the face image is very sensitive to small changes. This could be improved by adding a threshold for size changes.\n",
    "\n",
    "**4. Place face image at the right position** \\\n",
    "We use the `x_min` and `y_min` coordinates to replace the empty overlay with the face image starting at this x and y position.\n",
    "Note that the overlay image has the same dimesions as the camera image, whilst the face images has the same size as the detected face bounding box. To prevent out-of-bound errors, we use a smaller cut-out porition of the face image if we are at the borders (x and y smaller that 0 or size of face image overlaps camera image). That makes the code a little messy in this part.\n",
    "\n",
    "**5. Combine images** \\\n",
    "Now we have two images: One is the camera image, called `background`, the other one is the face images put into the empty overlay, called `foreground_with_alpha`. They both have the same size. \\\n",
    "In `combine_images` we perform the last step, returning them together as one image, with the face image on top and underlaying the camera image. To do this, we create an alpha mask out of the foreground image. This alpha masks contains 0 where the image is transparent, and ones where it is not. \\\n",
    "Our goal is to have `background` cut out where the face image should be positioned, and `foreground_with_alpha` everywhere else except where the actual face is. We realize that by simply multiplying each color channel with the alpha mask and the inverted alpha mask respectively. \\\n",
    "Combining both images by adding them, gives us the correct overlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_face_overlays(image, face_images, result):\n",
    "    if result.detections is None:\n",
    "        return image\n",
    "\n",
    "    for idx, detection in enumerate(result.detections):\n",
    "        face_img_no = idx % len(face_images)\n",
    "        image = add_face_overlay(image, face_images[face_img_no], detection)\n",
    "    return image\n",
    "\n",
    "def combine_images(background, foreground_with_alpha):\n",
    "    output_image_per_channel = 0\n",
    "    alpha_mask = np.divide(foreground_with_alpha[:, :, 3], 255).astype(bool)\n",
    "    image_channels = []\n",
    "    combined_image = []\n",
    "\n",
    "    for channel in range(background.shape[2]):\n",
    "        cut_out_background = np.multiply(\n",
    "            background[:, :, channel], np.invert(alpha_mask))\n",
    "        cut_out_foreground = np.multiply(\n",
    "            foreground_with_alpha[:, :, channel], alpha_mask)\n",
    "        image_channels.append(cut_out_background + cut_out_foreground)\n",
    "\n",
    "    return (np.dstack(image_channels)).astype(np.uint8)\n",
    "\n",
    "\n",
    "def scale_image(image, desired_width):\n",
    "\n",
    "    width, height = image.shape[1], image.shape[0]\n",
    "    ratio = desired_width / width\n",
    "    dimension = (int(width * ratio), int(height * ratio))\n",
    "    return cv2.resize(image, dimension, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "def add_face_overlay(camera_image, face_img, detection):\n",
    "\n",
    "    # 1. create a second transparent image for the overlay so we can place harold\n",
    "    trans_overlay_image = np.zeros((camera_image.shape[0], camera_image.shape[1], 4))\n",
    "\n",
    "    # 2. extract the upper left corner point of the face recognition bounding box as well as the width.\n",
    "    face_width = int(\n",
    "        detection.location_data.relative_bounding_box.width * camera_image.shape[1]) + 10\n",
    "    x_min = int((1 - detection.location_data.relative_bounding_box.xmin)\n",
    "                * camera_image.shape[1] - face_width) + 10\n",
    "    y_min = int(\n",
    "        detection.location_data.relative_bounding_box.ymin * camera_image.shape[0]) - 40\n",
    "\n",
    "    # 3. scale face image to the right width of the detected face\n",
    "    face_img = scale_image(face_img, face_width)\n",
    "\n",
    "    width_end = (face_img.shape[1]+x_min)\n",
    "    height_end = (face_img.shape[0]+y_min)\n",
    "\n",
    "    # handle replacing when at right and lower border\n",
    "    if width_end > camera_image.shape[1]:\n",
    "        width_end = camera_image.shape[1]\n",
    "    if height_end > camera_image.shape[0]:\n",
    "        height_end = camera_image.shape[0]\n",
    "\n",
    "    # handle replacing if we are at left and upper border\n",
    "    start_y = y_min\n",
    "    start_x = x_min\n",
    "    start_x_face = 0\n",
    "    start_y_face = 0\n",
    "    if x_min < 0:\n",
    "        start_x = 0\n",
    "        start_x_face = abs(x_min)\n",
    "    if y_min < 0:\n",
    "        start_y = 0\n",
    "        start_y_face = abs(y_min)\n",
    "\n",
    "    # 4. plug in face image at right position in overlay image\n",
    "    trans_overlay_image[start_y:height_end, start_x:width_end, :] = face_img[start_y_face:height_end +\n",
    "                                                                             start_y_face - start_y, start_x_face:width_end+start_x_face - start_x, :]\n",
    "    # 5. combine overlay image and original camera frame to one image and return it                                                                             \n",
    "    return combine_images(camera_image, trans_overlay_image)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now we can execute and test our script. We initialize the mediapipe face detection and open a camera stream with our webcam usign this snippet: `cv2.VideoCapture(0)`. If your camera does not open, try a different channel. \\\n",
    "Then in a while loop, we first run the face recognition detection, add the overlay and then show each frame using `cv2.imshow`. End the camera stream by pressing **q**.\n",
    "feel free to add more face overlays. They have to be transparent but are not restricted to a certain shape. Simply load them at the beginning and add them to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# read overlay images\n",
    "overlay_img_list = []\n",
    "overlay_img = cv2.imread(\"harold.png\", cv2.IMREAD_UNCHANGED)\n",
    "overlay_img = np.fliplr(overlay_img)\n",
    "overlay_img_list.append(overlay_img)\n",
    "\n",
    "try:\n",
    "    with  mp_face_detection.FaceDetection(\n",
    "        model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
    "        while cap.isOpened():\n",
    "            rel, frame = cap.read()\n",
    "            image, result = detection(frame, face_detection)\n",
    "            # draw_face_detection(image, result, mp_drawing)\n",
    "            # mirror the image to make it appear more natural in webcam\n",
    "            image =  np.fliplr(image)\n",
    "\n",
    "            image = add_face_overlays(image, overlay_img_list, result)\n",
    "            \n",
    "\n",
    "            cv2.imshow('OpenCV Frame', image)\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further improvements\n",
    "\n",
    "This face filter is, as stated, very very basic. There are some improvements which I might add some day, but propably never...\n",
    "* Detect rotation of the face and rotate filter accordingly\n",
    "* Make the size of the filter a little less sensitive to small face width changes\n",
    "* Sometimes, face detection detects nothing for a short period. Make overlay less sensitive to this as well\n",
    "* Use more keypoints, like eyes and nose to match the face overlay keypoints with the detected keypoints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0dd8442ba6cc7ce745fffab424ce0c652c6fa71b0ee16b804f7fb3c2d0872b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
